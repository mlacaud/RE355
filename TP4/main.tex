\documentclass[a4paper,11pt]{exam}
\usepackage[utf8]{inputenc}
\usepackage{enumerate}

%\usepackage{lipsum}

\date{12 November 2018}
\title{RE355 - Introduction to Cloud Networking\\
Lab session 4: Advanced issues in cloud computing}
\author{Simon Da Silva \and Mathias Lacaud}

%\usepackage{fancyhdr}
\usepackage{listings}
\usepackage{graphicx}

%\pagestyle{fancy}
%\fancyhf{}
\rhead{Bordeaux INP: ENSEIRB-MATMECA}
\lhead{RE355 - Introduction to Cloud Networking}

\lstset{language=sh,basicstyle=\ttfamily,columns=fullflexible}
\begin{document}

\maketitle

\section{Introduction - Setup}

This lab session deals with the use of Docker Swarm and additional tools to discover the issues with file sharing and elasticity in distributed systems. 
As it is the final lab session, you are supposed to know the basics.
We will expect you to search and find information by yourselves in the official documentations.
For this session, you need to work in groups with 3 to 4 computers on the same local network.

First of all, reinitialize all the machines:

\begin{lstlisting}[frame=single,language={sh}]  % Start your code-block

$ razrezo
$ date --set "$(curl https://time.akamai.com/?iso)"
$ docker swarm leave
$ docker system prune
$ service docker restart 

\end{lstlisting}

Be sure that chromium is installed in at least one of your computers: 

\begin{lstlisting}[frame=single,language={sh}]  % Start your code-block

$ apt install chromium
	
\end{lstlisting}

Pull some required images on all the computers:

\begin{lstlisting}[frame=single,language={sh}]  % Start your code-block

$ docker pull mlacaud/frontend_re355 
$ docker pull mlacaud/worker_re355 
$ docker pull mlacaud/server_re355

\end{lstlisting}

Then, create your swarm: 

\begin{lstlisting}[frame=single,language={sh}]  % Start your code-block

$ docker swarm init

\end{lstlisting}

Join the swarm with 3 or 4 machines.

Download the correction of the 2nd lab session from Moodle and extract it (using \texttt{unzip}). 
Go into the extracted folder and modify the published port of the frontend service in the compose file, by exposing port 80 instead of 3000 (it should look like \textit{80:3000} instead of \textit{3000:3000}).

Once done, start the application:

\begin{lstlisting}[frame=single,language={sh}]  % Start your code-block

$ docker stack deploy --compose-file docker-compose.yml <stack-name>

\end{lstlisting}

After a few seconds, you should be able to see your running application at the address \textbf{http://127.0.0.1:3000/} in chromium:

\begin{lstlisting}[frame=single,language={sh}]  % Start your code-block

$ chromium --no-sandbox http://127.0.0.1/ &
		
\end{lstlisting}

\clearpage

\section{Volumes and distributed file systems}

\subsection{File systems in the application}

Two application modules (inside containers) are storing informations on their local file system. 
The \textbf{Server} is storing the raw video waiting to be encoded, and the encoded video to be served, in its folder \textit{/go/app/fileServers}. 
The \textbf{Storage} (i.e. the database) is storing data in its folder \textit{/var/lib/mysql}. 

\begin{figure}[!ht]
	\centering
	\includegraphics[width=0.8\textwidth]{fig/architecture.png}
	\label{fig:architecture}
\end{figure}

\subsection{The problem with volumes...}

This section introduces the problem of volumes in Docker.

\subsubsection{Safe replication}

\begin{questions}
	\question Using \textbf{docker service}, create 3 replicas of the frontend.
	\begin{enumerate}[(a)]
		\item Refresh the web page several times. Do you see any change in the page? Does the application still work from different Frontend containers?
		\item Why is the replication of the Frontend working?
	\end{enumerate}
\end{questions}

\subsubsection{Critical replication}

\begin{questions}
	\question Using \textbf{docker service}, create 3 replicas of the server.
	\begin{enumerate}[(a)]
		\item Refresh the web page several times. Is the application still working every time?
		\item Knowing that the video data are stored on the file system of the server, what is happening?
		\item Docker Swarm does not include any tool to create distributed volumes. Do you know any tool to create distributed file systems?
	\end{enumerate}
\end{questions}

\subsubsection{Persistence in the database}

\begin{questions}
	\question Using \textbf{docker service}, create 3 replicas of the database.
	\begin{enumerate}[(a)]
		\item Refresh the web page several times. Is the application still working every time?
		\item Why?
	\end{enumerate}
\end{questions}

\clearpage

\subsection{Network file system}

As Docker Swarm and many orchestrators do not have embedded solution for volume management across the nodes, it should be handled by the user.
This section is about the creation of a distributed file system simultaneously mounted on every computers of the cluster using \textbf{NFS} (as a simple example).

\subsubsection{NFS Server}

First of all, install NFS:

\begin{lstlisting}[frame=single,language={sh}]  % Start your code-block

$ apt install nfs-kernel-server
			
\end{lstlisting}

Modify the file /etc/exports and export 2 folders of the server.

\begin{lstlisting}[frame=single,language={sh}]  % Start your code-block

/nfs_server *(rw,sync,fsid=0,no_root_squash,subtree_check)
/nfs_database *(rw,sync,fsid=0,no_root_squash,subtree_check)	
			
\end{lstlisting}

Start the nfs and rpcbind service: 

\begin{lstlisting}[frame=single,language={sh}]  % Start your code-block

service nfs-kernel-server start	
service rpcbind start			

\end{lstlisting}

\subsubsection{Volume mounted on the nodes}

\begin{questions}
	\question Mount the file system on the other computers.
	\begin{enumerate}[(a)]
		\item What command do you use?
		\item How can you test if the distributed file system is working?
	\end{enumerate}
	\question Bind the volumes to the server and database service in the \textit{docker-compose.yml} file.
	\begin{enumerate}[(a)]
		\item What lines did you add?
		\item Is the application fully functional when the server is replicated now?
	\end{enumerate}
\end{questions}

\subsubsection{Volume mounted on the services}

\begin{questions}
	\question In the \textit{docker-compose.yml} file, create a docker volume connected with the nfs-server. 
You can use the official documentation\footnote{https://docs.docker.com/engine/reference/commandline/volume/}.
	\begin{enumerate}[(a)]
		\item What did you add in the docker-compose.yml file?
		\item What is the difference with the previous solution?
	\end{enumerate}
	\question Bind the volumes on the server and database service in the \textit{docker-compose.yml}.
	\begin{enumerate}[(a)]
		\item What lines did you add?
		\item Is the application fully functional when the server is replicated now?
	\end{enumerate}
\end{questions}

This part about volumes, distributed file systems and data management in containers clusters is over.
The final section will introduce one major cloud computing engineering issue, scaling, which (we hope) you will enjoy solving!

\clearpage

\section{Monitoring tools and scaling}

This section deals with the monitoring of the cluster. 
The goal is to get some metrics on CPU, RAM and Bandwidth usage of computers, containers and services in our cluster, to scale them accordingly.

\subsection{Manual scaling}

We are going to install interesting monitoring tools like Prometheus, node-exporter, cAdvisor and Grafana. 
Those tools are very powerful but can be hard to install, understand and configure.
In order to simplify the process and see the basics, we are going to use a starter kit with a working docker compose file.

Download the source code:

\begin{lstlisting}[frame=single,language={sh}]  % Start your code-block

$ curl https://github.com/stefanprodan/swarmprom/archive/master.zip >  \
swarmprom.zip
$ unzip swarmprom.zip

\end{lstlisting}

On the manager node, deploy the stack using: 

\begin{lstlisting}[frame=single,language={sh}]  % Start your code-block

$ cd swarmprom-master
$ ADMIN_USER=admin \
 ADMIN_PASSWORD=admin \
 docker stack deploy -c docker-compose.yml monitoring
 
\end{lstlisting}

\begin{questions}

	\question Explain quickly with your own words:
	\begin{enumerate}[(a)]
		\item What is cAdvisor?
		\item What is node-exporter?
		\item What is Prometheus?
		\item What is Grafana?
	\end{enumerate}
	
	\question Go to \textbf{http://127.0.0.1:3000/} to see the Grafana interface.
	\begin{enumerate}[(a)]
		\item What are the credentials to log in the interface?
		\item What type of information do you have access to?
	\end{enumerate}
	
	\question Start a video transcoding from the frontend.
	\begin{enumerate}[(a)]
		\item Discuss the CPU usage. Can you identify on which computer the video is being transcoded?
		\item Discuss the other metrics.
	\end{enumerate}

	\question Start several video transcoding from the frontend.
	\begin{enumerate}[(a)]
		\item What do you see?  
		\item Scale the worker up to 3 replicas. What do you see now? Discuss the parallelization of the transcoding.
	\end{enumerate}
	
\end{questions}

\clearpage

\subsection{Automatic scaling}

In this section, which voluntarily contains few indications and explanations, you will deploy your own automatic scaling solution.

\subsubsection{Setup}

We will use the video encoding application as an example.
Remove all running containers by executing the command on ALL machines:

\begin{lstlisting}[frame=single,language={sh}]  % Start your code-block

docker rm -f $(docker ps -a -q)

\end{lstlisting}

Then deploy the application with one replica of each service:

\begin{lstlisting}[frame=single,language={sh}]  % Start your code-block

$ docker stack deploy --compose-file docker-compose.yml {stack-name}

\end{lstlisting}

If you upload multiple videos, they will be queued and encoded sequentially by the worker.
When you scale the worker service, multiple videos will be handled and encoded simultaneously.

\subsubsection{Problem}

Docker Stack (thanks to Docker Swarm) handles service load balancing across nodes.
When you manually scale your service (up or down), the containers are distributed on multiple machines according to the chosen policy (random, round robin, etc.), and requests are routed to the "most relevant" available container.

However, there is an issue remaining.
Even though load balancing is natively handled by Docker Swarm, automatic scaling is not.
It means that when multiple clients reach the service simultaneously, it will not scale by itself to reach high availability and quality of service.

\subsubsection{Goal}

Your mission, should you choose to accept it, is to automatically scale the worker service according to system / network / Docker metrics, in order to dynamically adapt the number of replicas to the number of queued videos.

There is no constraint on how you manage to reach this goal.
You can either implement complex monitoring tools, create an application to interact with the Swarm, or develop scripts for instance.
It's up to you!
Just make sure to explain your thought process in the report, and talk with us about your ideas.
Show your solution to your teacher, he might give you a cookie!

\subsubsection{Useful tools}

To do so, here are some tools that might help you in your struggle:

\begin{itemize}

\item RabbitMQ Management Plugin\footnote{https://www.rabbitmq.com/management.html}

\item Prometheus\footnote{https://prometheus.io/docs/introduction/overview/}

\item Docker Stats\footnote{https://docs.docker.com/engine/reference/commandline/stats/}

\end{itemize}

\subsubsection{Solution?}

We have developed a \textbf{perfect} solution that we shall demonstrate at the end of the lab session.

\end{document}
